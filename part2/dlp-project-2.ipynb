{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8b684b",
   "metadata": {},
   "source": [
    "# Comparative Analysis of Sequence-to-Sequence Models for Urdu Poetry Generation\n",
    "\n",
    "**Deep Learning for Perception - Semester Project Part 2**\n",
    "\n",
    "**Project Requirements:**\n",
    "- Dataset: Urdu Poetry Dataset from Hugging Face\n",
    "- Models: SimpleRNN, LSTM, Transformer\n",
    "- Optimizers: Adam, RMSprop, SGD\n",
    "- Total Experiments: 9 combinations (3 models Ã— 3 optimizers)\n",
    "- Hyperparameter Tuning: OFAT methodology\n",
    "- Output: Structured results for LaTeX report\n",
    "\n",
    "**Notebook Structure:**\n",
    "1. STEP 1: Dataset Loading & Exploration\n",
    "2. STEP 2: Data Preprocessing & Tokenization\n",
    "3. STEP 3: Model Architecture Definitions\n",
    "4. STEP 4: Optimizer Comparison (9 experiments)\n",
    "5. STEP 5: Hyperparameter Tuning\n",
    "6. STEP 6: Text Generation & Evaluation\n",
    "7. STEP 7: Results Export & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcedee8",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Implementation Notes\n",
    "\n",
    "**Based on Experimental Results Analysis:**\n",
    "- Initial models showed poor performance (4-9% accuracy, 700-1100 perplexity)\n",
    "- Root cause: Vocabulary too large for dataset size (1300 poems)\n",
    "- Solution: Reduced complexity, optimized for limited data\n",
    "\n",
    "**Applied Optimizations:**\n",
    "- Vocabulary: 1000 tokens (min_freq=5)\n",
    "- Embedding: 128 dimensions\n",
    "- RNN/LSTM units: 256\n",
    "- Sequence length: 20 tokens\n",
    "- Dropout: 0.1 (reduced from 0.2)\n",
    "- Training: 10 epochs, batch_size=64\n",
    "- Early stopping: patience=3\n",
    "\n",
    "**Expected Performance:**\n",
    "- Accuracy: 20-40% (improved from 4-9%)\n",
    "- Perplexity: 100-300 (improved from 700-1100)\n",
    "- Training Speed: 2-3x faster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55df010",
   "metadata": {},
   "source": [
    "## STEP 0: Environment Setup\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2882c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install pandas matplotlib scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05499f69",
   "metadata": {},
   "source": [
    "## STEP 1: Dataset Loading & Exploration\n",
    "\n",
    "Load the Urdu poetry dataset and perform basic exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "666257c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T14:57:23.592236Z",
     "iopub.status.busy": "2025-11-23T14:57:23.591941Z",
     "iopub.status.idle": "2025-11-23T14:57:23.634325Z",
     "shell.execute_reply": "2025-11-23T14:57:23.633537Z",
     "shell.execute_reply.started": "2025-11-23T14:57:23.592214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Overview ===\n",
      "Shape: (1323, 2)\n",
      "Columns: ['title', 'content']\n",
      "\n",
      "=== Dataset Structure ===\n",
      "Total poems: 1323\n",
      "Missing values: 9\n",
      "Duplicate poems: 8\n",
      "\n",
      "Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/kaggle/input/poetry/output_ur.csv\")\n",
    "\n",
    "# Basic exploration\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Basic dataset info\n",
    "poem_col = 'content'\n",
    "print(\"\\n=== Dataset Structure ===\")\n",
    "print(f\"Total poems: {len(df)}\")\n",
    "print(f\"Missing values: {df[poem_col].isnull().sum()}\")\n",
    "print(f\"Duplicate poems: {df.duplicated(subset=[poem_col]).sum()}\")\n",
    "print(\"\\nData loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56191417",
   "metadata": {},
   "source": [
    "## STEP 2: Data Preprocessing & Tokenization\n",
    "\n",
    "Clean, normalize, tokenize, and create sequences with strict 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "835b1694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T14:57:37.726186Z",
     "iopub.status.busy": "2025-11-23T14:57:37.725405Z",
     "iopub.status.idle": "2025-11-23T14:57:37.998942Z",
     "shell.execute_reply": "2025-11-23T14:57:37.998134Z",
     "shell.execute_reply.started": "2025-11-23T14:57:37.726162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "Starting data preprocessing...\n",
      "Removing missing values...\n",
      "Removed 9 rows with missing values\n",
      "Removing duplicate poems...\n",
      "Removed 0 duplicate poems\n",
      "Cleaning text...\n",
      "Normalizing Urdu text...\n",
      "Filtering empty texts...\n",
      "Tokenizing text...\n",
      "Preprocessing complete. Remaining samples: 1314\n",
      "Building vocabulary...\n",
      "Vocabulary limited to top 1000 most frequent tokens\n",
      "Vocabulary size: 1004 (including special tokens)\n",
      "Special tokens: ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
      "Preparing sequences...\n",
      "Splitting data...\n",
      "Train set: 1051 samples\n",
      "Validation set: 131 samples\n",
      "Test set: 132 samples\n",
      "Preprocessing complete!\n",
      "Vocab: 1004 | Train: 1051 | Val: 131 | Test: 132\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize Urdu text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "\n",
    "    # Normalize Unicode (handle different representations of same characters)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "    # Remove control characters but keep Urdu characters\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "\n",
    "    # Handle common Urdu special characters - keep Urdu script and basic punctuation\n",
    "    # Remove non-Urdu characters except spaces and basic punctuation\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s\\.,!?Ø›:Û”]', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_urdu_text(text):\n",
    "    \"\"\"\n",
    "    Additional Urdu-specific normalization\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Handle Urdu specific character variations\n",
    "    # Normalize alef variants\n",
    "    text = re.sub(r'[Ø§Ø£Ø¥Ø¢]', 'Ø§', text)\n",
    "\n",
    "    # Normalize yeh variants\n",
    "    text = re.sub(r'[ÙŠÙ‰]', 'ÛŒ', text)\n",
    "\n",
    "    # Normalize heh variants\n",
    "    text = re.sub(r'[Ù‡Ø©]', 'Û', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize Urdu text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Simple tokenization by whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def preprocess_data(df, poem_col='content', remove_duplicates=True, remove_missing=True):\n",
    "    \"\"\"\n",
    "    Main preprocessing pipeline with improved data quality handling\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Remove missing values first\n",
    "    if remove_missing:\n",
    "        print(\"Removing missing values...\")\n",
    "        df = df.dropna(subset=[poem_col]).copy()\n",
    "        print(f\"Removed {initial_count - len(df)} rows with missing values\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    if remove_duplicates:\n",
    "        print(\"Removing duplicate poems...\")\n",
    "        before_dedup = len(df)\n",
    "        df = df.drop_duplicates(subset=[poem_col]).copy()\n",
    "        print(f\"Removed {before_dedup - len(df)} duplicate poems\")\n",
    "\n",
    "    # Clean text\n",
    "    print(\"Cleaning text...\")\n",
    "    df['cleaned_text'] = df[poem_col].apply(clean_text)\n",
    "\n",
    "    # Normalize Urdu text\n",
    "    print(\"Normalizing Urdu text...\")\n",
    "    df['normalized_text'] = df['cleaned_text'].apply(normalize_urdu_text)\n",
    "\n",
    "    # Filter empty lines\n",
    "    print(\"Filtering empty texts...\")\n",
    "    df = df[df['normalized_text'].str.len() > 0].copy()\n",
    "\n",
    "    # Tokenize\n",
    "    print(\"Tokenizing text...\")\n",
    "    df['tokens'] = df['normalized_text'].apply(tokenize_text)\n",
    "\n",
    "    # Filter out empty token lists and very short sequences\n",
    "    df = df[df['tokens'].str.len() > 5].copy()  # At least 5 tokens\n",
    "\n",
    "    print(f\"Preprocessing complete. Remaining samples: {len(df)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_vocabulary(tokens_list, min_freq=2, max_vocab_size=5000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from tokenized texts with frequency filtering\n",
    "    \"\"\"\n",
    "    print(\"Building vocabulary...\")\n",
    "\n",
    "    # Flatten all tokens\n",
    "    all_tokens = [token for tokens in tokens_list for token in tokens]\n",
    "\n",
    "    # Count frequencies\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Filter by minimum frequency\n",
    "    vocab = {token: count for token, count in token_counts.items() if count >= min_freq}\n",
    "\n",
    "    # Sort by frequency (most common first)\n",
    "    vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Limit vocabulary size (keep most frequent tokens)\n",
    "    if len(vocab) > max_vocab_size:\n",
    "        vocab = dict(list(vocab.items())[:max_vocab_size])\n",
    "        print(f\"Vocabulary limited to top {max_vocab_size} most frequent tokens\")\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = {\n",
    "        '<PAD>': 0,  # Padding token (use index 0 for padding)\n",
    "        '<UNK>': 1,  # Unknown token\n",
    "        '<SOS>': 2,  # Start of sequence\n",
    "        '<EOS>': 3   # End of sequence\n",
    "    }\n",
    "\n",
    "    # Create token to index mapping (start from 4 to reserve 0-3 for special tokens)\n",
    "    token_to_idx = special_tokens.copy()\n",
    "    for idx, token in enumerate(vocab.keys(), start=4):\n",
    "        token_to_idx[token] = idx\n",
    "\n",
    "    # Create index to token mapping\n",
    "    idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "    print(f\"Vocabulary size: {len(token_to_idx)} (including special tokens)\")\n",
    "    print(f\"Special tokens: {list(special_tokens.keys())}\")\n",
    "\n",
    "    return token_to_idx, idx_to_token, vocab\n",
    "\n",
    "def prepare_sequences(tokens_list, token_to_idx, max_length=None):\n",
    "    \"\"\"\n",
    "    Convert tokens to numerical sequences\n",
    "    \"\"\"\n",
    "    print(\"Preparing sequences...\")\n",
    "\n",
    "    sequences = []\n",
    "    for tokens in tokens_list:\n",
    "        # Convert tokens to indices\n",
    "        seq = [token_to_idx.get(token, token_to_idx['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Add start and end tokens\n",
    "        seq = [token_to_idx['<SOS>']] + seq + [token_to_idx['<EOS>']]\n",
    "\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def split_data(df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "\n",
    "    print(\"Splitting data...\")\n",
    "\n",
    "    # First split: train and temp (val+test)\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df,\n",
    "        train_size=train_ratio,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Second split: val and test from temp\n",
    "    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        train_size=val_ratio_adjusted,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Execute preprocessing with improved settings\n",
    "print(\"=\" * 60)\n",
    "print(\"IMPROVED PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "processed_df = preprocess_data(df, poem_col, remove_duplicates=True, remove_missing=True)\n",
    "\n",
    "# Build vocabulary - reduced size for better learning with limited data\n",
    "token_to_idx, idx_to_token, vocab = build_vocabulary(\n",
    "    processed_df['tokens'],\n",
    "    min_freq=5,  # Higher frequency threshold to reduce noise\n",
    "    max_vocab_size=1000  # Smaller vocab for 1300 poems\n",
    ")\n",
    "\n",
    "# Prepare sequences\n",
    "sequences = prepare_sequences(processed_df['tokens'], token_to_idx)\n",
    "processed_df['sequences'] = sequences\n",
    "\n",
    "# Split data\n",
    "train_df, val_df, test_df = split_data(processed_df)\n",
    "train_sequences = train_df['sequences'].tolist()\n",
    "val_sequences = val_df['sequences'].tolist()\n",
    "test_sequences = test_df['sequences'].tolist()\n",
    "\n",
    "# Save processed data\n",
    "processed_data = {\n",
    "    'train_sequences': train_sequences,\n",
    "    'val_sequences': val_sequences,\n",
    "    'test_sequences': test_sequences,\n",
    "    'token_to_idx': token_to_idx,\n",
    "    'idx_to_token': idx_to_token,\n",
    "    'vocab': vocab\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(f\"Vocab: {len(token_to_idx)} | Train: {len(train_sequences)} | Val: {len(val_sequences)} | Test: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932a7d1",
   "metadata": {},
   "source": [
    "## STEP 3: Model Architecture Definitions\n",
    "\n",
    "Define SimpleRNN, LSTM, and Transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e13ed141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T14:58:09.173676Z",
     "iopub.status.busy": "2025-11-23T14:58:09.172904Z",
     "iopub.status.idle": "2025-11-23T14:58:09.191731Z",
     "shell.execute_reply": "2025-11-23T14:58:09.190776Z",
     "shell.execute_reply.started": "2025-11-23T14:58:09.173647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model definitions loaded!\n",
      "Available models: ['rnn', 'lstm', 'transformer']\n",
      "Available optimizers: ['adam', 'rmsprop', 'sgd']\n",
      "\n",
      "Optimizations for small dataset (1300 poems):\n",
      "  â€¢ Reduced embedding: 256 â†’ 128 dimensions\n",
      "  â€¢ Reduced RNN/LSTM units: 512 â†’ 256\n",
      "  â€¢ Reduced sequence length: 40 â†’ 20 tokens\n",
      "  â€¢ Lower dropout: 0.2 â†’ 0.1 (was blocking learning)\n",
      "  â€¢ Transformer: 4 heads, 256 ff_dim, 2 blocks\n",
      "  â€¢ Faster training: 10 epochs, batch_size=64\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout, Input, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import numpy as np\n",
    "\n",
    "def create_rnn_model(vocab_size, embedding_dim, rnn_units, seq_length):\n",
    "    \"\"\"\n",
    "    Create improved RNN-based model for text generation\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=seq_length, mask_zero=True),\n",
    "        SimpleRNN(rnn_units, return_sequences=True, dropout=0.1, recurrent_dropout=0.1),\n",
    "        SimpleRNN(rnn_units, dropout=0.1, recurrent_dropout=0.1),\n",
    "        Dense(rnn_units // 2, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(vocab_size, embedding_dim, rnn_units, seq_length):\n",
    "    \"\"\"\n",
    "    Create improved LSTM-based model for text generation\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=seq_length, mask_zero=True),\n",
    "        LSTM(rnn_units, return_sequences=True, dropout=0.1, recurrent_dropout=0.0),\n",
    "        LSTM(rnn_units, dropout=0.1, recurrent_dropout=0.0),\n",
    "        Dense(rnn_units // 2, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_transformer_block(x, embed_dim, num_heads, ff_dim, dropout_rate=0.05):\n",
    "    \"\"\"\n",
    "    Transformer encoder block\n",
    "    \"\"\"\n",
    "    # Multi-head attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)(x, x)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "    # Feed-forward network (must output same dimension as input)\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)  # Project back to embed_dim\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "    return out2\n",
    "\n",
    "def create_transformer_model(vocab_size, embedding_dim, num_heads, ff_dim, num_blocks, seq_length, dropout_rate=0.05):\n",
    "    \"\"\"\n",
    "    Create improved Transformer-based model for text generation\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(seq_length,))\n",
    "\n",
    "    # Embedding layer\n",
    "    x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
    "\n",
    "    # Positional encoding (simplified)\n",
    "    positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "    positional_encoding = Embedding(seq_length, embedding_dim)(positions)\n",
    "    x = x + positional_encoding\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_blocks):\n",
    "        x = create_transformer_block(x, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "    # Global pooling and output\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(ff_dim, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_optimizer(optimizer_name, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Get optimizer by name with appropriate settings\n",
    "    \"\"\"\n",
    "    optimizers = {\n",
    "        'adam': Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
    "        'rmsprop': RMSprop(learning_rate=learning_rate, clipnorm=1.0),\n",
    "        'sgd': SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True, clipnorm=1.0)\n",
    "    }\n",
    "\n",
    "    return optimizers.get(optimizer_name.lower(), Adam(learning_rate=learning_rate))\n",
    "\n",
    "def create_model(model_type, vocab_size, embedding_dim, rnn_units, seq_length,\n",
    "                num_heads=8, ff_dim=128, num_blocks=2):\n",
    "    \"\"\"\n",
    "    Factory function to create models\n",
    "    \"\"\"\n",
    "    if model_type.lower() == 'rnn':\n",
    "        return create_rnn_model(vocab_size, embedding_dim, rnn_units, seq_length)\n",
    "    elif model_type.lower() == 'lstm':\n",
    "        return create_lstm_model(vocab_size, embedding_dim, rnn_units, seq_length)\n",
    "    elif model_type.lower() == 'transformer':\n",
    "        return create_transformer_model(vocab_size, embedding_dim, num_heads, ff_dim, num_blocks, seq_length)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "def compile_model(model, optimizer_name, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Compile model with specified optimizer\n",
    "    \"\"\"\n",
    "    optimizer = get_optimizer(optimizer_name, learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Optimized model configurations for small dataset (1300 poems)\n",
    "MODEL_CONFIGS = {\n",
    "    'rnn': {\n",
    "        'embedding_dim': 128,  # Reduced to prevent overfitting\n",
    "        'rnn_units': 256,      # Reduced capacity\n",
    "        'seq_length': 20,      # Shorter sequences for faster training\n",
    "        'learning_rate': 0.001\n",
    "    },\n",
    "    'lstm': {\n",
    "        'embedding_dim': 128,\n",
    "        'rnn_units': 256,\n",
    "        'seq_length': 20,\n",
    "        'learning_rate': 0.001\n",
    "    },\n",
    "    'transformer': {\n",
    "        'embedding_dim': 128,\n",
    "        'num_heads': 4,        # Reduced from 8\n",
    "        'ff_dim': 256,         # Reduced from 512\n",
    "        'num_blocks': 2,       # Reduced from 3\n",
    "        'seq_length': 20,\n",
    "        'learning_rate': 0.001  # Increased for faster convergence\n",
    "    }\n",
    "}\n",
    "\n",
    "OPTIMIZERS = ['adam', 'rmsprop', 'sgd']\n",
    "\n",
    "print(\"Improved model definitions loaded!\")\n",
    "print(f\"Available models: {list(MODEL_CONFIGS.keys())}\")\n",
    "print(f\"Available optimizers: {OPTIMIZERS}\")\n",
    "print(\"\\nOptimizations for small dataset (1300 poems):\")\n",
    "print(\"  â€¢ Reduced embedding: 256 â†’ 128 dimensions\")\n",
    "print(\"  â€¢ Reduced RNN/LSTM units: 512 â†’ 256\")\n",
    "print(\"  â€¢ Reduced sequence length: 40 â†’ 20 tokens\")\n",
    "print(\"  â€¢ Lower dropout: 0.2 â†’ 0.1 (was blocking learning)\")\n",
    "print(\"  â€¢ Transformer: 4 heads, 256 ff_dim, 2 blocks\")\n",
    "print(\"  â€¢ Faster training: 20 epochs, batch_size=64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8eb870",
   "metadata": {},
   "source": [
    "## STEP 4: Optimizer Comparison (Core Experiments)\n",
    "\n",
    "Train and evaluate 9 model-optimizer combinations. Each experiment runs in a separate cell for progress tracking and memory management.\n",
    "\n",
    "**Required Combinations:**\n",
    "1. SimpleRNN + Adam\n",
    "2. SimpleRNN + RMSprop  \n",
    "3. SimpleRNN + SGD\n",
    "4. LSTM + Adam\n",
    "5. LSTM + RMSprop\n",
    "6. LSTM + SGD\n",
    "7. Transformer + Adam\n",
    "8. Transformer + RMSprop\n",
    "9. Transformer + SGD\n",
    "\n",
    "**Metrics Tracked:** Training time, accuracy, loss, perplexity (exp(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98fd0f67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:05:59.861222Z",
     "iopub.status.busy": "2025-11-23T15:05:59.860907Z",
     "iopub.status.idle": "2025-11-23T15:05:59.881758Z",
     "shell.execute_reply": "2025-11-23T15:05:59.880799Z",
     "shell.execute_reply.started": "2025-11-23T15:05:59.861198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities loaded!\n",
      "All experiment results will be stored in 'all_experiment_results' list\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# Global results storage for all experiments\n",
    "if 'all_experiment_results' not in globals():\n",
    "    all_experiment_results = []\n",
    "\n",
    "def prepare_training_data(sequences, seq_length, vocab_size):\n",
    "    \"\"\"Prepare input-target pairs with padding\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq) > seq_length + 1:\n",
    "            for i in range(len(seq) - seq_length):\n",
    "                X.append(seq[i:i+seq_length])\n",
    "                y.append(seq[i+seq_length])\n",
    "        elif len(seq) > 1:\n",
    "            padded_seq = seq[:-1]\n",
    "            if len(padded_seq) < seq_length:\n",
    "                padded_seq = padded_seq + [0] * (seq_length - len(padded_seq))\n",
    "            elif len(padded_seq) > seq_length:\n",
    "                padded_seq = padded_seq[:seq_length]\n",
    "            X.append(padded_seq)\n",
    "            y.append(seq[-1])\n",
    "    \n",
    "    X_padded = pad_sequences(X, maxlen=seq_length, padding='pre', value=0)\n",
    "    y_array = np.array(y)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_padded)}, Input shape: {X_padded.shape}\")\n",
    "    return X_padded, y_array\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"Calculate perplexity from cross-entropy loss\"\"\"\n",
    "    import math\n",
    "    try:\n",
    "        return math.exp(loss)\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "def train_and_evaluate(model_type, optimizer_name, train_sequences, val_sequences, test_sequences,\n",
    "                       token_to_idx, epochs=20, batch_size=64, learning_rate=None, experiment_name=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model-optimizer combination.\n",
    "    Tracks all metrics for CSV export.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"EXPERIMENT: {model_type.upper()} + {optimizer_name.upper()}\")\n",
    "    if experiment_name:\n",
    "        print(f\"Custom Config: {experiment_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    config = MODEL_CONFIGS[model_type.lower()]\n",
    "    vocab_size = len(token_to_idx)\n",
    "    seq_length = config['seq_length']\n",
    "    \n",
    "    if learning_rate is None:\n",
    "        learning_rate = config.get('learning_rate', 0.001)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    X_train, y_train = prepare_training_data(train_sequences, seq_length, vocab_size)\n",
    "    X_val, y_val = prepare_training_data(val_sequences, seq_length, vocab_size)\n",
    "    X_test, y_test = prepare_training_data(test_sequences, seq_length, vocab_size)\n",
    "    \n",
    "    # Create model\n",
    "    print(f\"\\nBuilding {model_type.upper()} model...\")\n",
    "    if model_type.lower() == 'transformer':\n",
    "        model = create_transformer_model(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ff_dim=config['ff_dim'],\n",
    "            num_blocks=config['num_blocks'],\n",
    "            seq_length=seq_length\n",
    "        )\n",
    "    else:\n",
    "        model = create_model(\n",
    "            model_type=model_type,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            rnn_units=config['rnn_units'],\n",
    "            seq_length=seq_length\n",
    "        )\n",
    "    \n",
    "    model = compile_model(model, optimizer_name, learning_rate)\n",
    "    \n",
    "    # Training callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nğŸ“Š Evaluating on test set...\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_perplexity = calculate_perplexity(test_loss)\n",
    "    \n",
    "    # Get final training metrics\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'Experiment_ID': f\"{model_type}_{optimizer_name}\" if not experiment_name else experiment_name,\n",
    "        'Model': model_type.upper(),\n",
    "        'Optimizer': optimizer_name.upper(),\n",
    "        'Hyperparams': f\"lr={learning_rate}, batch={batch_size}, epochs={epochs}\",\n",
    "        'Training_Time': round(training_time, 2),\n",
    "        'Final_Train_Loss': round(final_train_loss, 4),\n",
    "        'Final_Train_Accuracy': round(final_train_acc, 4),\n",
    "        'Final_Val_Loss': round(final_val_loss, 4),\n",
    "        'Final_Val_Accuracy': round(final_val_acc, 4),\n",
    "        'Test_Loss': round(test_loss, 4),\n",
    "        'Test_Accuracy': round(test_acc, 4),\n",
    "        'Test_Perplexity': round(test_perplexity, 4),\n",
    "        'Epochs_Trained': len(history.history['loss'])\n",
    "    }\n",
    "    \n",
    "    all_experiment_results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nCOMPLETED\")\n",
    "    print(f\"Training Time: {training_time:.2f}s ({training_time/60:.2f} min)\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Perplexity: {test_perplexity:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return model, history, result\n",
    "\n",
    "print(\"Training utilities loaded!\")\n",
    "print(\"All experiment results will be stored in 'all_experiment_results' list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448355b",
   "metadata": {},
   "source": [
    "### STEP 4.1: Training Utilities\n",
    "\n",
    "Helper functions for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c752c8",
   "metadata": {},
   "source": [
    "### STEP 4.2: Experiment 1 - SimpleRNN + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c2876a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:06:11.100677Z",
     "iopub.status.busy": "2025-11-23T15:06:11.100040Z",
     "iopub.status.idle": "2025-11-23T15:07:36.940399Z",
     "shell.execute_reply": "2025-11-23T15:07:36.939705Z",
     "shell.execute_reply.started": "2025-11-23T15:06:11.100652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: RNN + ADAM\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building RNN model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 10ms/step - accuracy: 0.1890 - loss: 5.2961 - val_accuracy: 0.1920 - val_loss: 5.1417\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1914 - loss: 5.1918 - val_accuracy: 0.1920 - val_loss: 5.1361\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1935 - loss: 5.1825 - val_accuracy: 0.1920 - val_loss: 5.1423\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1889 - loss: 5.1946 - val_accuracy: 0.1920 - val_loss: 5.1376\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1914 - loss: 5.1876 - val_accuracy: 0.1920 - val_loss: 5.1378\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 82.72s (1.38 min)\n",
      "Test Accuracy: 0.1868\n",
      "Test Loss: 5.1865\n",
      "Test Perplexity: 178.8432\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_rnn_adam, history_rnn_adam, result_rnn_adam = train_and_evaluate(\n",
    "    model_type='rnn',\n",
    "    optimizer_name='adam',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d3675",
   "metadata": {},
   "source": [
    "### STEP 4.3: Experiment 2 - SimpleRNN + RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69dd51a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:08:46.369080Z",
     "iopub.status.busy": "2025-11-23T15:08:46.368763Z",
     "iopub.status.idle": "2025-11-23T15:11:18.730799Z",
     "shell.execute_reply": "2025-11-23T15:11:18.729955Z",
     "shell.execute_reply.started": "2025-11-23T15:08:46.369058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: RNN + RMSPROP\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building RNN model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 12ms/step - accuracy: 0.1916 - loss: 5.2760 - val_accuracy: 0.1920 - val_loss: 5.4202\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1915 - loss: 5.2319 - val_accuracy: 0.1920 - val_loss: 5.3044\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1910 - loss: 5.2202 - val_accuracy: 0.1920 - val_loss: 5.1377\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1910 - loss: 5.1937 - val_accuracy: 0.1920 - val_loss: 5.1339\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1926 - loss: 5.1750 - val_accuracy: 0.1920 - val_loss: 5.1376\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1910 - loss: 5.1458 - val_accuracy: 0.1920 - val_loss: 5.1222\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1906 - loss: 5.1344 - val_accuracy: 0.1920 - val_loss: 5.1265\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1905 - loss: 5.1405 - val_accuracy: 0.1920 - val_loss: 5.1347\n",
      "Epoch 9/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1910 - loss: 5.1327 - val_accuracy: 0.1920 - val_loss: 5.1401\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 149.17s (2.49 min)\n",
      "Test Accuracy: 0.1868\n",
      "Test Loss: 5.1701\n",
      "Test Perplexity: 175.9400\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_rnn_rmsprop, history_rnn_rmsprop, result_rnn_rmsprop = train_and_evaluate(\n",
    "    model_type='rnn',\n",
    "    optimizer_name='rmsprop',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b7f09",
   "metadata": {},
   "source": [
    "### STEP 4.4: Experiment 3 - SimpleRNN + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49221e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:11:30.335340Z",
     "iopub.status.busy": "2025-11-23T15:11:30.335099Z",
     "iopub.status.idle": "2025-11-23T15:13:48.295387Z",
     "shell.execute_reply": "2025-11-23T15:13:48.294680Z",
     "shell.execute_reply.started": "2025-11-23T15:11:30.335324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: RNN + SGD\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building RNN model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - accuracy: 0.1751 - loss: 5.6583 - val_accuracy: 0.1920 - val_loss: 5.1475\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1895 - loss: 5.2003 - val_accuracy: 0.1920 - val_loss: 5.1307\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1903 - loss: 5.1736 - val_accuracy: 0.1920 - val_loss: 5.1272\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1902 - loss: 5.1688 - val_accuracy: 0.1920 - val_loss: 5.1260\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1919 - loss: 5.1486 - val_accuracy: 0.1920 - val_loss: 5.1246\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1912 - loss: 5.1487 - val_accuracy: 0.1920 - val_loss: 5.1229\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1892 - loss: 5.1556 - val_accuracy: 0.1920 - val_loss: 5.1241\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1912 - loss: 5.1447 - val_accuracy: 0.1920 - val_loss: 5.1243\n",
      "Epoch 9/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.1909 - loss: 5.1493 - val_accuracy: 0.1920 - val_loss: 5.1254\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 134.82s (2.25 min)\n",
      "Test Accuracy: 0.1868\n",
      "Test Loss: 5.1731\n",
      "Test Perplexity: 176.4564\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_rnn_sgd, history_rnn_sgd, result_rnn_sgd = train_and_evaluate(\n",
    "    model_type='rnn',\n",
    "    optimizer_name='sgd',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402a1c2",
   "metadata": {},
   "source": [
    "### STEP 4.5: Experiment 4 - LSTM + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47115ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:14:23.391010Z",
     "iopub.status.busy": "2025-11-23T15:14:23.390206Z",
     "iopub.status.idle": "2025-11-23T15:19:06.671861Z",
     "shell.execute_reply": "2025-11-23T15:19:06.671122Z",
     "shell.execute_reply.started": "2025-11-23T15:14:23.390982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: LSTM + ADAM\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.1880 - loss: 5.2650 - val_accuracy: 0.1928 - val_loss: 5.0073\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.1927 - loss: 5.0205 - val_accuracy: 0.1973 - val_loss: 4.9597\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.1985 - loss: 4.9459 - val_accuracy: 0.2036 - val_loss: 4.8980\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2052 - loss: 4.8656 - val_accuracy: 0.2042 - val_loss: 4.8623\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.2072 - loss: 4.8214 - val_accuracy: 0.2050 - val_loss: 4.8355\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2069 - loss: 4.7745 - val_accuracy: 0.2051 - val_loss: 4.8048\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2087 - loss: 4.7221 - val_accuracy: 0.2042 - val_loss: 4.7891\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2086 - loss: 4.6817 - val_accuracy: 0.2072 - val_loss: 4.7778\n",
      "Epoch 9/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2132 - loss: 4.6439 - val_accuracy: 0.2066 - val_loss: 4.7647\n",
      "Epoch 10/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2138 - loss: 4.5981 - val_accuracy: 0.2077 - val_loss: 4.7628\n",
      "Epoch 11/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2138 - loss: 4.5731 - val_accuracy: 0.2076 - val_loss: 4.7619\n",
      "Epoch 12/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.2178 - loss: 4.5278 - val_accuracy: 0.2075 - val_loss: 4.7708\n",
      "Epoch 13/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.2178 - loss: 4.4958 - val_accuracy: 0.2074 - val_loss: 4.7811\n",
      "Epoch 14/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.2211 - loss: 4.4453 - val_accuracy: 0.2082 - val_loss: 4.7900\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 280.35s (4.67 min)\n",
      "Test Accuracy: 0.2065\n",
      "Test Loss: 4.7920\n",
      "Test Perplexity: 120.5435\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_lstm_adam, history_lstm_adam, result_lstm_adam = train_and_evaluate(\n",
    "    model_type='lstm',\n",
    "    optimizer_name='adam',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9cfcf",
   "metadata": {},
   "source": [
    "### STEP 4.6: Experiment 5 - LSTM + RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8800f105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:20:25.586874Z",
     "iopub.status.busy": "2025-11-23T15:20:25.586526Z",
     "iopub.status.idle": "2025-11-23T15:23:38.644661Z",
     "shell.execute_reply": "2025-11-23T15:23:38.643900Z",
     "shell.execute_reply.started": "2025-11-23T15:20:25.586851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: LSTM + RMSPROP\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.1912 - loss: 5.2560 - val_accuracy: 0.1920 - val_loss: 5.1511\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.1907 - loss: 5.1276 - val_accuracy: 0.1932 - val_loss: 5.0219\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.1932 - loss: 5.0060 - val_accuracy: 0.1938 - val_loss: 4.9587\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.1957 - loss: 4.9334 - val_accuracy: 0.2003 - val_loss: 4.9033\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.2038 - loss: 4.8749 - val_accuracy: 0.2037 - val_loss: 4.8621\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.2064 - loss: 4.8116 - val_accuracy: 0.2061 - val_loss: 4.8418\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.2095 - loss: 4.7943 - val_accuracy: 0.2064 - val_loss: 4.8201\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.2082 - loss: 4.7679 - val_accuracy: 0.2040 - val_loss: 4.8270\n",
      "Epoch 9/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.2154 - loss: 4.7291 - val_accuracy: 0.2065 - val_loss: 4.8217\n",
      "Epoch 10/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.2148 - loss: 4.7265 - val_accuracy: 0.2031 - val_loss: 4.8205\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 190.11s (3.17 min)\n",
      "Test Accuracy: 0.2050\n",
      "Test Loss: 4.8407\n",
      "Test Perplexity: 126.5593\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_lstm_rmsprop, history_lstm_rmsprop, result_lstm_rmsprop = train_and_evaluate(\n",
    "    model_type='lstm',\n",
    "    optimizer_name='rmsprop',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ab839",
   "metadata": {},
   "source": [
    "### STEP 4.7: Experiment 6 - LSTM + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a1e707a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:24:35.557491Z",
     "iopub.status.busy": "2025-11-23T15:24:35.556892Z",
     "iopub.status.idle": "2025-11-23T15:26:23.897128Z",
     "shell.execute_reply": "2025-11-23T15:26:23.896434Z",
     "shell.execute_reply.started": "2025-11-23T15:24:35.557448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: LSTM + SGD\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.1876 - loss: 6.2726 - val_accuracy: 0.1920 - val_loss: 5.1669\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.1894 - loss: 5.2071 - val_accuracy: 0.1920 - val_loss: 5.1302\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.1926 - loss: 5.1556 - val_accuracy: 0.1920 - val_loss: 5.1232\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.1897 - loss: 5.1606 - val_accuracy: 0.1920 - val_loss: 5.1265\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.1905 - loss: 5.1535 - val_accuracy: 0.1920 - val_loss: 5.1245\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.1912 - loss: 5.1474 - val_accuracy: 0.1920 - val_loss: 5.1235\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 105.39s (1.76 min)\n",
      "Test Accuracy: 0.1868\n",
      "Test Loss: 5.1751\n",
      "Test Perplexity: 176.8076\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_lstm_sgd, history_lstm_sgd, result_lstm_sgd = train_and_evaluate(\n",
    "    model_type='lstm',\n",
    "    optimizer_name='sgd',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93090ee",
   "metadata": {},
   "source": [
    "### STEP 4.8: Experiment 7 - Transformer + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23cd4f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:26:29.214222Z",
     "iopub.status.busy": "2025-11-23T15:26:29.213928Z",
     "iopub.status.idle": "2025-11-23T15:30:34.492714Z",
     "shell.execute_reply": "2025-11-23T15:30:34.491909Z",
     "shell.execute_reply.started": "2025-11-23T15:26:29.214182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: TRANSFORMER + ADAM\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building TRANSFORMER model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 11ms/step - accuracy: 0.1896 - loss: 5.2425 - val_accuracy: 0.1920 - val_loss: 5.1234\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1921 - loss: 5.1184 - val_accuracy: 0.1920 - val_loss: 5.1203\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1896 - loss: 5.1084 - val_accuracy: 0.1920 - val_loss: 5.0962\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1929 - loss: 5.0570 - val_accuracy: 0.1920 - val_loss: 5.0903\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1903 - loss: 5.0630 - val_accuracy: 0.1920 - val_loss: 5.0971\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1905 - loss: 5.0482 - val_accuracy: 0.1920 - val_loss: 5.0925\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1911 - loss: 5.0317 - val_accuracy: 0.1920 - val_loss: 5.0726\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1912 - loss: 5.0014 - val_accuracy: 0.1932 - val_loss: 5.0282\n",
      "Epoch 9/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1946 - loss: 4.9574 - val_accuracy: 0.1947 - val_loss: 5.0036\n",
      "Epoch 10/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1965 - loss: 4.9259 - val_accuracy: 0.1953 - val_loss: 4.9812\n",
      "Epoch 11/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1974 - loss: 4.9006 - val_accuracy: 0.1962 - val_loss: 4.9617\n",
      "Epoch 12/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1963 - loss: 4.8673 - val_accuracy: 0.1959 - val_loss: 4.9475\n",
      "Epoch 13/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1972 - loss: 4.8360 - val_accuracy: 0.1970 - val_loss: 4.9429\n",
      "Epoch 14/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1978 - loss: 4.8106 - val_accuracy: 0.1950 - val_loss: 4.9363\n",
      "Epoch 15/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1961 - loss: 4.7962 - val_accuracy: 0.1953 - val_loss: 4.9338\n",
      "Epoch 16/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1965 - loss: 4.7874 - val_accuracy: 0.1966 - val_loss: 4.9306\n",
      "Epoch 17/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1964 - loss: 4.7824 - val_accuracy: 0.1962 - val_loss: 4.9487\n",
      "Epoch 18/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1956 - loss: 4.7625 - val_accuracy: 0.1967 - val_loss: 4.9388\n",
      "Epoch 19/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1989 - loss: 4.7545 - val_accuracy: 0.1957 - val_loss: 4.9361\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 241.70s (4.03 min)\n",
      "Test Accuracy: 0.1960\n",
      "Test Loss: 4.9537\n",
      "Test Perplexity: 141.7023\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_transformer_adam, history_transformer_adam, result_transformer_adam = train_and_evaluate(\n",
    "    model_type='transformer',\n",
    "    optimizer_name='adam',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2bf87",
   "metadata": {},
   "source": [
    "### STEP 4.9: Experiment 8 - Transformer + RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e94a33a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:31:06.885060Z",
     "iopub.status.busy": "2025-11-23T15:31:06.884311Z",
     "iopub.status.idle": "2025-11-23T15:33:07.704806Z",
     "shell.execute_reply": "2025-11-23T15:33:07.703950Z",
     "shell.execute_reply.started": "2025-11-23T15:31:06.885032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: TRANSFORMER + RMSPROP\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building TRANSFORMER model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 11ms/step - accuracy: 0.1895 - loss: 5.2152 - val_accuracy: 0.1917 - val_loss: 5.0448\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.2019 - loss: 4.8986 - val_accuracy: 0.2053 - val_loss: 4.8443\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.2093 - loss: 4.7735 - val_accuracy: 0.2042 - val_loss: 4.8322\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.2108 - loss: 4.7470 - val_accuracy: 0.2054 - val_loss: 4.8483\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.2150 - loss: 4.7234 - val_accuracy: 0.2042 - val_loss: 4.8145\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.2167 - loss: 4.7052 - val_accuracy: 0.2056 - val_loss: 4.8175\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.2181 - loss: 4.6762 - val_accuracy: 0.2049 - val_loss: 4.8145\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.2208 - loss: 4.6628 - val_accuracy: 0.2054 - val_loss: 4.8213\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 117.75s (1.96 min)\n",
      "Test Accuracy: 0.2057\n",
      "Test Loss: 4.8319\n",
      "Test Perplexity: 125.4443\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_transformer_rmsprop, history_transformer_rmsprop, result_transformer_rmsprop = train_and_evaluate(\n",
    "    model_type='transformer',\n",
    "    optimizer_name='rmsprop',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b68566",
   "metadata": {},
   "source": [
    "### STEP 4.10: Experiment 9 - Transformer + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00ee9a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:35:33.663344Z",
     "iopub.status.busy": "2025-11-23T15:35:33.662597Z",
     "iopub.status.idle": "2025-11-23T15:39:43.182607Z",
     "shell.execute_reply": "2025-11-23T15:39:43.181862Z",
     "shell.execute_reply.started": "2025-11-23T15:35:33.663319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: TRANSFORMER + SGD\n",
      "================================================================================\n",
      "\n",
      "Preparing data...\n",
      "Training samples: 120070, Input shape: (120070, 20)\n",
      "Training samples: 15271, Input shape: (15271, 20)\n",
      "Training samples: 14232, Input shape: (14232, 20)\n",
      "\n",
      "Building TRANSFORMER model...\n",
      "\n",
      "Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - accuracy: 0.1865 - loss: 5.5081 - val_accuracy: 0.1920 - val_loss: 5.1388\n",
      "Epoch 2/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1921 - loss: 5.1520 - val_accuracy: 0.1920 - val_loss: 5.1261\n",
      "Epoch 3/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1919 - loss: 5.1318 - val_accuracy: 0.1920 - val_loss: 5.1191\n",
      "Epoch 4/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1913 - loss: 5.1346 - val_accuracy: 0.1920 - val_loss: 5.1243\n",
      "Epoch 5/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1912 - loss: 5.1276 - val_accuracy: 0.1920 - val_loss: 5.1190\n",
      "Epoch 6/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1926 - loss: 5.1306 - val_accuracy: 0.1920 - val_loss: 5.1190\n",
      "Epoch 7/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1903 - loss: 5.1315 - val_accuracy: 0.1920 - val_loss: 5.1111\n",
      "Epoch 8/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1914 - loss: 5.1207 - val_accuracy: 0.1920 - val_loss: 5.1048\n",
      "Epoch 9/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1906 - loss: 5.1140 - val_accuracy: 0.1920 - val_loss: 5.1046\n",
      "Epoch 10/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1931 - loss: 5.0925 - val_accuracy: 0.1920 - val_loss: 5.0979\n",
      "Epoch 11/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1902 - loss: 5.1115 - val_accuracy: 0.1920 - val_loss: 5.1018\n",
      "Epoch 12/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1902 - loss: 5.1034 - val_accuracy: 0.1920 - val_loss: 5.0906\n",
      "Epoch 13/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1901 - loss: 5.0840 - val_accuracy: 0.1919 - val_loss: 5.0877\n",
      "Epoch 14/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.1916 - loss: 5.0787 - val_accuracy: 0.1920 - val_loss: 5.0818\n",
      "Epoch 15/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1905 - loss: 5.0773 - val_accuracy: 0.1919 - val_loss: 5.0786\n",
      "Epoch 16/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1905 - loss: 5.0612 - val_accuracy: 0.1919 - val_loss: 5.0777\n",
      "Epoch 17/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1901 - loss: 5.0556 - val_accuracy: 0.1920 - val_loss: 5.0720\n",
      "Epoch 18/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1915 - loss: 5.0371 - val_accuracy: 0.1920 - val_loss: 5.0667\n",
      "Epoch 19/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1910 - loss: 5.0355 - val_accuracy: 0.1920 - val_loss: 5.0580\n",
      "Epoch 20/20\n",
      "\u001b[1m1877/1877\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1911 - loss: 5.0291 - val_accuracy: 0.1920 - val_loss: 5.0554\n",
      "\n",
      "ğŸ“Š Evaluating on test set...\n",
      "\n",
      "COMPLETED\n",
      "Training Time: 246.49s (4.11 min)\n",
      "Test Accuracy: 0.1867\n",
      "Test Loss: 5.0948\n",
      "Test Perplexity: 163.1741\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_transformer_sgd, history_transformer_sgd, result_transformer_sgd = train_and_evaluate(\n",
    "    model_type='transformer',\n",
    "    optimizer_name='sgd',\n",
    "    train_sequences=train_sequences,\n",
    "    val_sequences=val_sequences,\n",
    "    test_sequences=test_sequences,\n",
    "    token_to_idx=token_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c80b5",
   "metadata": {},
   "source": [
    "### STEP 4.11: Core Experiments Summary\n",
    "\n",
    "View all 9 experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f07448d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:39:46.692054Z",
     "iopub.status.busy": "2025-11-23T15:39:46.691766Z",
     "iopub.status.idle": "2025-11-23T15:39:46.712083Z",
     "shell.execute_reply": "2025-11-23T15:39:46.711175Z",
     "shell.execute_reply.started": "2025-11-23T15:39:46.692033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ CORE EXPERIMENTS SUMMARY\n",
      "================================================================================\n",
      "Total experiments completed: 14\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Results Table:\n",
      "      Experiment_ID  Test_Accuracy  Test_Perplexity  Training_Time\n",
      "           rnn_adam         0.1868         178.3094         128.27\n",
      "        rnn_rmsprop         0.1868         176.0376         140.64\n",
      "            rnn_sgd         0.1868         176.4497         101.93\n",
      "           rnn_adam         0.1868         178.3588         110.48\n",
      "        rnn_rmsprop         0.1868         175.9705         150.27\n",
      "           rnn_adam         0.1868         178.8432          82.72\n",
      "        rnn_rmsprop         0.1868         175.9400         149.17\n",
      "            rnn_sgd         0.1868         176.4564         134.82\n",
      "          lstm_adam         0.2065         120.5435         280.35\n",
      "       lstm_rmsprop         0.2050         126.5593         190.11\n",
      "           lstm_sgd         0.1868         176.8076         105.39\n",
      "   transformer_adam         0.1960         141.7023         241.70\n",
      "transformer_rmsprop         0.2057         125.4443         117.75\n",
      "    transformer_sgd         0.1867         163.1741         246.49\n",
      "\n",
      "ğŸ† BEST MODEL: lstm_adam\n",
      "   Accuracy: 0.2065\n",
      "   Perplexity: 120.5435\n",
      "   Training Time: 280.35s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display core experiments summary\n",
    "import pandas as pd\n",
    "\n",
    "print(\"ğŸ¯ CORE EXPERIMENTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total experiments completed: {len(all_experiment_results)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if all_experiment_results:\n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(all_experiment_results)\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\nğŸ“Š Results Table:\")\n",
    "    print(df_results[['Experiment_ID', 'Test_Accuracy', 'Test_Perplexity', 'Training_Time']].to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = df_results['Test_Accuracy'].idxmax()\n",
    "    best_model = df_results.iloc[best_idx]\n",
    "    \n",
    "    print(f\"\\nğŸ† BEST MODEL: {best_model['Experiment_ID']}\")\n",
    "    print(f\"   Accuracy: {best_model['Test_Accuracy']:.4f}\")\n",
    "    print(f\"   Perplexity: {best_model['Test_Perplexity']:.4f}\")\n",
    "    print(f\"   Training Time: {best_model['Training_Time']:.2f}s\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No experiments completed yet. Run the experiment cells above.\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef934c54-8048-4df0-b4b1-cc3232498e7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16dfd999",
   "metadata": {},
   "source": [
    "## STEP 5: Hyperparameter Experimentation\n",
    "\n",
    "Systematic hyperparameter tuning using One-Factor-At-A-Time (OFAT) methodology.\n",
    "\n",
    "**OFAT Methodology:**\n",
    "- Start with the best model from Step 4 as baseline\n",
    "- Change one hyperparameter at a time while keeping others fixed\n",
    "- Test specified values from syllabus for each parameter\n",
    "\n",
    "**Parameters to Tune:**\n",
    "1. **Architecture:** `num_layers` [1, 2, 3], `dropout_rate` [0.1, 0.2, 0.3, 0.5]\n",
    "2. **Training:** `learning_rate` [0.0001, 0.001, 0.01, 0.1], `batch_size` [32, 64, 128, 256], `epochs` [10, 20, 30, 50], `seq_length` [10, 15, 20, 30]\n",
    "3. **Transformer-specific (if best model is Transformer):** `num_heads` [2, 4, 8], `ff_dim` [256, 512, 1024], `num_blocks` [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Initialize hyperparameter results storage\n",
    "if 'hyperparam_results' not in globals():\n",
    "    hyperparam_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 5: HYPERPARAMETER EXPERIMENTATION - OFAT METHODOLOGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Identify best model from Step 4\n",
    "if 'all_experiment_results' in globals() and all_experiment_results:\n",
    "    df_core = pd.DataFrame(all_experiment_results)\n",
    "    best_idx = df_core['Test_Accuracy'].idxmax()\n",
    "    best_config = df_core.iloc[best_idx]\n",
    "    \n",
    "    print(\"\\nBest Model from Core Experiments:\")\n",
    "    print(f\"  Experiment ID: {best_config['Experiment_ID']}\")\n",
    "    print(f\"  Model: {best_config['Model']}\")\n",
    "    print(f\"  Optimizer: {best_config['Optimizer']}\")\n",
    "    print(f\"  Test Accuracy: {best_config['Test_Accuracy']:.4f}\")\n",
    "    print(f\"  Test Perplexity: {best_config['Test_Perplexity']:.4f}\")\n",
    "    print(f\"  Training Time: {best_config['Training_Time']:.2f}s\")\n",
    "    \n",
    "    # Extract baseline configuration\n",
    "    best_model_type = best_config['Model'].lower()\n",
    "    best_optimizer = best_config['Optimizer'].lower()\n",
    "    \n",
    "    # Get baseline config from MODEL_CONFIGS\n",
    "    baseline_config = MODEL_CONFIGS[best_model_type].copy()\n",
    "    baseline_config['optimizer'] = best_optimizer\n",
    "    baseline_config['batch_size'] = 64  # Default from Step 4\n",
    "    baseline_config['epochs'] = 10      # Default from Step 4\n",
    "    \n",
    "    print(\"\\nBaseline Configuration:\")\n",
    "    for key, value in baseline_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nERROR: No core experiment results found.\")\n",
    "    print(\"Please run STEP 4 before running STEP 5.\")\n",
    "    best_model_type = None\n",
    "    best_optimizer = None\n",
    "    baseline_config = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c86919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for OFAT experiments\n",
    "def run_ofat_experiment(param_name, param_value, config, model_type, optimizer_name, \n",
    "                        X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Run a single OFAT experiment with specified parameter value.\n",
    "    \n",
    "    Args:\n",
    "        param_name: Name of the parameter being tuned\n",
    "        param_value: Value to test for the parameter\n",
    "        config: Configuration dictionary with all parameters\n",
    "        model_type: Model architecture type\n",
    "        optimizer_name: Optimizer to use\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test: Training data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with experiment results\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Testing {param_name}={param_value}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Apply parameter value to config\n",
    "        test_config = config.copy()\n",
    "        test_config[param_name] = param_value\n",
    "        \n",
    "        # Special handling for different parameters\n",
    "        batch_size = test_config.get('batch_size', 64)\n",
    "        epochs = test_config.get('epochs', 10)\n",
    "        learning_rate = test_config.get('learning_rate', 0.001)\n",
    "        \n",
    "        # Create model based on architecture\n",
    "        with strategy.scope():\n",
    "            vocab_size = len(token_to_idx)\n",
    "            \n",
    "            if model_type == 'transformer':\n",
    "                model = create_transformer_model(\n",
    "                    vocab_size=vocab_size,\n",
    "                    embedding_dim=test_config.get('embedding_dim', 128),\n",
    "                    num_heads=test_config.get('num_heads', 4),\n",
    "                    ff_dim=test_config.get('ff_dim', 256),\n",
    "                    num_blocks=test_config.get('num_blocks', 2),\n",
    "                    seq_length=test_config.get('seq_length', 20),\n",
    "                    dropout_rate=test_config.get('dropout_rate', 0.05)\n",
    "                )\n",
    "            else:\n",
    "                # For RNN/LSTM with num_layers support\n",
    "                if param_name == 'num_layers':\n",
    "                    num_layers = param_value\n",
    "                    dropout_rate = test_config.get('dropout_rate', 0.1)\n",
    "                    \n",
    "                    model = Sequential([Embedding(vocab_size, test_config['embedding_dim'], \n",
    "                                                 input_length=test_config['seq_length'], mask_zero=True)])\n",
    "                    \n",
    "                    # Add RNN/LSTM layers\n",
    "                    for i in range(num_layers):\n",
    "                        return_sequences = (i < num_layers - 1)  # Last layer doesn't return sequences\n",
    "                        if model_type == 'rnn':\n",
    "                            model.add(SimpleRNN(test_config['rnn_units'], return_sequences=return_sequences,\n",
    "                                              dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "                        else:  # lstm\n",
    "                            model.add(LSTM(test_config['rnn_units'], return_sequences=return_sequences,\n",
    "                                         dropout=dropout_rate, recurrent_dropout=0.0))\n",
    "                    \n",
    "                    model.add(Dense(test_config['rnn_units'] // 2, activation='relu'))\n",
    "                    model.add(Dropout(dropout_rate))\n",
    "                    model.add(Dense(vocab_size, activation='softmax'))\n",
    "                    \n",
    "                elif param_name == 'dropout_rate':\n",
    "                    # Rebuild model with new dropout\n",
    "                    dropout_rate = param_value\n",
    "                    num_layers = test_config.get('num_layers', 2)\n",
    "                    \n",
    "                    model = Sequential([Embedding(vocab_size, test_config['embedding_dim'],\n",
    "                                                 input_length=test_config['seq_length'], mask_zero=True)])\n",
    "                    \n",
    "                    for i in range(num_layers):\n",
    "                        return_sequences = (i < num_layers - 1)\n",
    "                        if model_type == 'rnn':\n",
    "                            model.add(SimpleRNN(test_config['rnn_units'], return_sequences=return_sequences,\n",
    "                                              dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "                        else:\n",
    "                            model.add(LSTM(test_config['rnn_units'], return_sequences=return_sequences,\n",
    "                                         dropout=dropout_rate, recurrent_dropout=0.0))\n",
    "                    \n",
    "                    model.add(Dense(test_config['rnn_units'] // 2, activation='relu'))\n",
    "                    model.add(Dropout(dropout_rate))\n",
    "                    model.add(Dense(vocab_size, activation='softmax'))\n",
    "                else:\n",
    "                    # Use standard create_model function\n",
    "                    model = create_model(\n",
    "                        model_type=model_type,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embedding_dim=test_config.get('embedding_dim', 128),\n",
    "                        rnn_units=test_config.get('rnn_units', 256),\n",
    "                        seq_length=test_config.get('seq_length', 20),\n",
    "                        num_heads=test_config.get('num_heads', 4),\n",
    "                        ff_dim=test_config.get('ff_dim', 256),\n",
    "                        num_blocks=test_config.get('num_blocks', 2)\n",
    "                    )\n",
    "            \n",
    "            # Compile model\n",
    "            model = compile_model(model, optimizer_name, learning_rate)\n",
    "        \n",
    "        # Train model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, \n",
    "                                      restore_best_weights=True, verbose=0)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_perplexity = np.exp(test_loss)\n",
    "        \n",
    "        print(f\"Acc={test_accuracy:.4f}, Perp={test_perplexity:.2f}, Time={training_time:.1f}s\")\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            'Parameter': param_name,\n",
    "            'Value': param_value,\n",
    "            'Model': model_type.upper(),\n",
    "            'Optimizer': optimizer_name.upper(),\n",
    "            'Test_Accuracy': round(test_accuracy, 4),\n",
    "            'Test_Loss': round(test_loss, 4),\n",
    "            'Test_Perplexity': round(test_perplexity, 4),\n",
    "            'Training_Time': round(training_time, 2),\n",
    "            'Epochs_Trained': len(history.history['loss'])\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"OFAT experiment function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97682c5f",
   "metadata": {},
   "source": [
    "### OFAT Experiment 1: Number of Layers\n",
    "\n",
    "Test different numbers of layers for the RNN/LSTM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 1: Number of Layers\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [1, 2, 3]\")\n",
    "    print(f\"Baseline: num_layers=2 (default)\")\n",
    "    \n",
    "    # Prepare data with baseline seq_length\n",
    "    seq_length = baseline_config['seq_length']\n",
    "    vocab_size = len(token_to_idx)\n",
    "    X_train, y_train = prepare_training_data(train_sequences, seq_length, vocab_size)\n",
    "    X_val, y_val = prepare_training_data(val_sequences, seq_length, vocab_size)\n",
    "    X_test, y_test = prepare_training_data(test_sequences, seq_length, vocab_size)\n",
    "    \n",
    "    # Add num_layers to baseline config\n",
    "    baseline_config['num_layers'] = 2  # Default value\n",
    "    \n",
    "    # Test each value\n",
    "    for num_layers in [1, 2, 3]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='num_layers',\n",
    "            param_value=num_layers,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'num_layers'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06876f0b",
   "metadata": {},
   "source": [
    "### OFAT Experiment 2: Dropout Rate\n",
    "\n",
    "Test different dropout rates for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ce637",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 2: Dropout Rate\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [0.1, 0.2, 0.3, 0.5]\")\n",
    "    print(f\"Baseline: dropout_rate=0.1\")\n",
    "    \n",
    "    # Add dropout_rate to baseline config\n",
    "    baseline_config['dropout_rate'] = 0.1  # Default value\n",
    "    \n",
    "    # Test each value\n",
    "    for dropout_rate in [0.1, 0.2, 0.3, 0.5]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='dropout_rate',\n",
    "            param_value=dropout_rate,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'dropout_rate'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebffc2d",
   "metadata": {},
   "source": [
    "### OFAT Experiment 3: Learning Rate\n",
    "\n",
    "Test different learning rates for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad50182",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 3: Learning Rate\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [0.0001, 0.001, 0.01, 0.1]\")\n",
    "    print(f\"Baseline: learning_rate={baseline_config['learning_rate']}\")\n",
    "    \n",
    "    # Test each value\n",
    "    for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='learning_rate',\n",
    "            param_value=lr,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'learning_rate'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef699d0",
   "metadata": {},
   "source": [
    "### OFAT Experiment 4: Batch Size\n",
    "\n",
    "Test different batch sizes for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 4: Batch Size\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [32, 64, 128, 256]\")\n",
    "    print(f\"Baseline: batch_size={baseline_config['batch_size']}\")\n",
    "    \n",
    "    # Test each value\n",
    "    for batch_size in [32, 64, 128, 256]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='batch_size',\n",
    "            param_value=batch_size,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'batch_size'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4394f84",
   "metadata": {},
   "source": [
    "### OFAT Experiment 5: Training Epochs\n",
    "\n",
    "Test different numbers of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655084fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 5: Training Epochs\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [10, 20, 30, 50]\")\n",
    "    print(f\"Baseline: epochs={baseline_config['epochs']}\")\n",
    "    \n",
    "    # Test each value\n",
    "    for epochs in [10, 20, 30, 50]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='epochs',\n",
    "            param_value=epochs,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'epochs'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6388c",
   "metadata": {},
   "source": [
    "### OFAT Experiment 6: Sequence Length\n",
    "\n",
    "Test different sequence lengths. **Note:** This requires re-generating training data for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 6: Sequence Length\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [10, 15, 20, 30]\")\n",
    "    print(f\"Baseline: seq_length={baseline_config['seq_length']}\")\n",
    "    print(\"Note: Re-generating training data for each sequence length...\")\n",
    "    \n",
    "    # Test each value\n",
    "    for seq_len in [10, 15, 20, 30]:\n",
    "        print(f\"\\n  Preparing data for seq_length={seq_len}...\", end=\" \")\n",
    "        \n",
    "        # Re-generate training data with new sequence length\n",
    "        X_train_seq, y_train_seq = prepare_training_data(train_sequences, seq_len, vocab_size)\n",
    "        X_val_seq, y_val_seq = prepare_training_data(val_sequences, seq_len, vocab_size)\n",
    "        X_test_seq, y_test_seq = prepare_training_data(test_sequences, seq_len, vocab_size)\n",
    "        \n",
    "        print(\"Done.\")\n",
    "        \n",
    "        result = run_ofat_experiment(\n",
    "            param_name='seq_length',\n",
    "            param_value=seq_len,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train_seq, y_train=y_train_seq,\n",
    "            X_val=X_val_seq, y_val=y_val_seq,\n",
    "            X_test=X_test_seq, y_test=y_test_seq\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'seq_length'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b766de8",
   "metadata": {},
   "source": [
    "### OFAT Experiment 7: Transformer-Specific - Number of Attention Heads\n",
    "\n",
    "**Only runs if the best model is a Transformer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9591ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type == 'transformer' and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 7: Number of Attention Heads (Transformer)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [2, 4, 8]\")\n",
    "    print(f\"Baseline: num_heads={baseline_config.get('num_heads', 4)}\")\n",
    "    \n",
    "    # Reset data to baseline seq_length\n",
    "    seq_length = baseline_config['seq_length']\n",
    "    X_train, y_train = prepare_training_data(train_sequences, seq_length, vocab_size)\n",
    "    X_val, y_val = prepare_training_data(val_sequences, seq_length, vocab_size)\n",
    "    X_test, y_test = prepare_training_data(test_sequences, seq_length, vocab_size)\n",
    "    \n",
    "    # Test each value\n",
    "    for num_heads in [2, 4, 8]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='num_heads',\n",
    "            param_value=num_heads,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'num_heads'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "elif best_model_type and best_model_type != 'transformer':\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 7: Number of Attention Heads (Transformer)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Skipping: Best model is {best_model_type.upper()}, not Transformer.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cdcd7",
   "metadata": {},
   "source": [
    "### OFAT Experiment 8: Transformer-Specific - Feed-Forward Dimension\n",
    "\n",
    "**Only runs if the best model is a Transformer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type == 'transformer' and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 8: Feed-Forward Dimension (Transformer)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [256, 512, 1024]\")\n",
    "    print(f\"Baseline: ff_dim={baseline_config.get('ff_dim', 256)}\")\n",
    "    \n",
    "    # Test each value\n",
    "    for ff_dim in [256, 512, 1024]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='ff_dim',\n",
    "            param_value=ff_dim,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'ff_dim'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "elif best_model_type and best_model_type != 'transformer':\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 8: Feed-Forward Dimension (Transformer)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Skipping: Best model is {best_model_type.upper()}, not Transformer.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa6af8",
   "metadata": {},
   "source": [
    "### OFAT Experiment 9: Transformer-Specific - Number of Transformer Blocks\n",
    "\n",
    "**Only runs if the best model is a Transformer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type == 'transformer' and baseline_config:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 9: Number of Transformer Blocks\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Testing values: [1, 2, 3, 4]\")\n",
    "    print(f\"Baseline: num_blocks={baseline_config.get('num_blocks', 2)}\")\n",
    "    \n",
    "    # Test each value\n",
    "    for num_blocks in [1, 2, 3, 4]:\n",
    "        result = run_ofat_experiment(\n",
    "            param_name='num_blocks',\n",
    "            param_value=num_blocks,\n",
    "            config=baseline_config,\n",
    "            model_type=best_model_type,\n",
    "            optimizer_name=best_optimizer,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            hyperparam_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Completed: {len([r for r in hyperparam_results if r['Parameter'] == 'num_blocks'])} experiments\")\n",
    "    print(\"=\"*80)\n",
    "elif best_model_type and best_model_type != 'transformer':\n",
    "    print(\"=\"*80)\n",
    "    print(\"OFAT EXPERIMENT 9: Number of Transformer Blocks\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Skipping: Best model is {best_model_type.upper()}, not Transformer.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping: Best model configuration not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c688ba2",
   "metadata": {},
   "source": [
    "### OFAT Summary: Analysis and Export\n",
    "\n",
    "Display all hyperparameter tuning results and export to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING SUMMARY - OFAT METHODOLOGY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_hyperparam = pd.DataFrame(hyperparam_results)\n",
    "    \n",
    "    print(f\"\\nTotal experiments completed: {len(df_hyperparam)}\")\n",
    "    print(f\"Parameters tuned: {df_hyperparam['Parameter'].unique().tolist()}\")\n",
    "    \n",
    "    # Display all results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All Hyperparameter Tuning Results:\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_hyperparam[['Parameter', 'Value', 'Test_Accuracy', 'Test_Perplexity', \n",
    "                         'Training_Time']].to_string(index=False))\n",
    "    \n",
    "    # Find best value for each parameter\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Best Value for Each Parameter:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for param in df_hyperparam['Parameter'].unique():\n",
    "        param_df = df_hyperparam[df_hyperparam['Parameter'] == param]\n",
    "        best_idx = param_df['Test_Accuracy'].idxmax()\n",
    "        best_row = param_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n{param}:\")\n",
    "        print(f\"  Best Value: {best_row['Value']}\")\n",
    "        print(f\"  Test Accuracy: {best_row['Test_Accuracy']:.4f}\")\n",
    "        print(f\"  Test Perplexity: {best_row['Test_Perplexity']:.2f}\")\n",
    "        print(f\"  Training Time: {best_row['Training_Time']:.2f}s\")\n",
    "    \n",
    "    # Overall best configuration\n",
    "    overall_best_idx = df_hyperparam['Test_Accuracy'].idxmax()\n",
    "    overall_best = df_hyperparam.iloc[overall_best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Overall Best Single Configuration from OFAT:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Parameter: {overall_best['Parameter']}\")\n",
    "    print(f\"Value: {overall_best['Value']}\")\n",
    "    print(f\"Model: {overall_best['Model']}\")\n",
    "    print(f\"Optimizer: {overall_best['Optimizer']}\")\n",
    "    print(f\"Test Accuracy: {overall_best['Test_Accuracy']:.4f}\")\n",
    "    print(f\"Test Perplexity: {overall_best['Test_Perplexity']:.2f}\")\n",
    "    print(f\"Training Time: {overall_best['Training_Time']:.2f}s\")\n",
    "    \n",
    "    # Export to CSV\n",
    "    csv_path = 'hyperparameter_tuning_results.csv'\n",
    "    df_hyperparam.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Results exported to: {csv_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nSTEP 5 COMPLETE: Hyperparameter experimentation finished successfully.\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"No hyperparameter tuning results available.\")\n",
    "    print(\"Please run the OFAT experiments above.\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33805d8a",
   "metadata": {},
   "source": [
    "## STEP 6: Text Generation\n",
    "\n",
    "Generate Urdu poetry using trained models with specific seed words and temperature variations.\n",
    "\n",
    "**Requirements:**\n",
    "- **Seed Words (Urdu):**\n",
    "  - Ù…Ø­Ø¨Øª (Love)\n",
    "  - Ø¯Ù„ (Heart)\n",
    "  - Ø´Ø§Ù… (Evening)\n",
    "  - ÛŒØ§Ø¯ (Memory)\n",
    "  - Ø®ÙˆØ´ÛŒ (Happiness)\n",
    "\n",
    "- **Temperature Settings:** 0.7 (Conservative), 1.0 (Balanced), 1.3 (Creative)\n",
    "- **Models:** Generate text from all 9 trained models\n",
    "- **Output Length:** 30-50 tokens per generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd63f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Text Generation with All Required Seed Words and Temperatures\n",
    "\n",
    "# Define requirements\n",
    "SEED_WORDS = [\n",
    "    'Ù…Ø­Ø¨Øª',  # Love\n",
    "    'Ø¯Ù„',    # Heart\n",
    "    'Ø´Ø§Ù…',   # Evening  \n",
    "    'ÛŒØ§Ø¯',   # Memory\n",
    "    'Ø®ÙˆØ´ÛŒ'   # Happiness\n",
    "]\n",
    "\n",
    "TEMPERATURES = [0.7, 1.0, 1.3]  # Conservative, Balanced, Creative\n",
    "MAX_LENGTH = 50  # tokens per generation\n",
    "\n",
    "# Store all generation results\n",
    "generation_results = []\n",
    "\n",
    "# Get all trained models\n",
    "trained_models = [\n",
    "    ('SimpleRNN + Adam', model_rnn_adam if 'model_rnn_adam' in globals() else None),\n",
    "    ('SimpleRNN + RMSprop', model_rnn_rmsprop if 'model_rnn_rmsprop' in globals() else None),\n",
    "    ('SimpleRNN + SGD', model_rnn_sgd if 'model_rnn_sgd' in globals() else None),\n",
    "    ('LSTM + Adam', model_lstm_adam if 'model_lstm_adam' in globals() else None),\n",
    "    ('LSTM + RMSprop', model_lstm_rmsprop if 'model_lstm_rmsprop' in globals() else None),\n",
    "    ('LSTM + SGD', model_lstm_sgd if 'model_lstm_sgd' in globals() else None),\n",
    "    ('Transformer + Adam', model_transformer_adam if 'model_transformer_adam' in globals() else None),\n",
    "    ('Transformer + RMSprop', model_transformer_rmsprop if 'model_transformer_rmsprop' in globals() else None),\n",
    "    ('Transformer + SGD', model_transformer_sgd if 'model_transformer_sgd' in globals() else None),\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¨ TEXT GENERATION - Urdu Poetry\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nGenerating with {len(SEED_WORDS)} seed words Ã— {len(TEMPERATURES)} temperatures\")\n",
    "print(f\"per model = {len(SEED_WORDS) * len(TEMPERATURES)} generations per model\\n\")\n",
    "\n",
    "# Generate text for each model\n",
    "for model_name, model in trained_models:\n",
    "    if model is None:\n",
    "        print(f\"\\nâš ï¸  {model_name}: Model not trained yet, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“ {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for seed in SEED_WORDS:\n",
    "        print(f\"\\nğŸŒ± Seed: {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for temp in TEMPERATURES:\n",
    "            try:\n",
    "                generated = generate_text(\n",
    "                    model=model,\n",
    "                    idx_to_token=idx_to_token,\n",
    "                    token_to_idx=token_to_idx,\n",
    "                    seed_text=seed,\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    temperature=temp\n",
    "                )\n",
    "                \n",
    "                # Store result\n",
    "                generation_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Seed_Word': seed,\n",
    "                    'Temperature': temp,\n",
    "                    'Generated_Text': generated,\n",
    "                    'Length': len(generated.split())\n",
    "                })\n",
    "                \n",
    "                # Display\n",
    "                print(f\"  T={temp}: {generated[:80]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  T={temp}: Error - {e}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Text Generation Complete!\")\n",
    "print(f\"Total generations: {len(generation_results)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bdcc5",
   "metadata": {},
   "source": [
    "## STEP 7: Results Export & Visualization\n",
    "\n",
    "Export all experiment results to CSV and create visualizations for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b825b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all experiment results to CSV\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ’¾ EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Export Core Experiment Results\n",
    "if all_experiment_results:\n",
    "    df_experiments = pd.DataFrame(all_experiment_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = 'data/experiment_results.csv'\n",
    "    df_experiments.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nâœ… Core experiments exported to: {csv_path}\")\n",
    "    print(f\"   Columns: {list(df_experiments.columns)}\")\n",
    "    print(f\"   Rows: {len(df_experiments)}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nğŸ“Š RESULTS SUMMARY:\")\n",
    "    print(df_experiments[['Experiment_ID', 'Test_Accuracy', 'Test_Perplexity', 'Training_Time']].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No core experiment results to export\")\n",
    "\n",
    "# 2. Export Text Generation Results\n",
    "if 'generation_results' in globals() and generation_results:\n",
    "    df_generation = pd.DataFrame(generation_results)\n",
    "    gen_csv_path = 'data/text_generation_results.csv'\n",
    "    df_generation.to_csv(gen_csv_path, index=False)\n",
    "    print(f\"\\nâœ… Text generations exported to: {gen_csv_path}\")\n",
    "    print(f\"   Total generations: {len(df_generation)}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No text generation results to export\")\n",
    "\n",
    "# 3. Export Hyperparameter Tuning Results (if available)\n",
    "if 'hyperparam_results' in globals() and hyperparam_results:\n",
    "    df_hyperparam = pd.DataFrame(hyperparam_results)\n",
    "    hyper_csv_path = 'data/hyperparameter_tuning_results.csv'\n",
    "    df_hyperparam.to_csv(hyper_csv_path, index=False)\n",
    "    print(f\"\\nâœ… Hyperparameter tuning exported to: {hyper_csv_path}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No hyperparameter tuning results to export\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ CREATING VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create visualizations if we have results\n",
    "if all_experiment_results:\n",
    "    # Set up plotting style\n",
    "    plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available else 'default')\n",
    "    \n",
    "    # Figure 1: Test Accuracy Comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    df_plot = pd.DataFrame(all_experiment_results)\n",
    "    models = df_plot['Experiment_ID']\n",
    "    accuracies = df_plot['Test_Accuracy']\n",
    "    perplexities = df_plot['Test_Perplexity']\n",
    "    \n",
    "    # Plot 1: Accuracy\n",
    "    bars1 = ax1.barh(models, accuracies, color='skyblue')\n",
    "    ax1.set_xlabel('Test Accuracy', fontsize=12)\n",
    "    ax1.set_title('Model Comparison - Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlim([0, max(accuracies) * 1.1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Perplexity\n",
    "    bars2 = ax2.barh(models, perplexities, color='lightcoral')\n",
    "    ax2.set_xlabel('Test Perplexity', fontsize=12)\n",
    "    ax2.set_title('Model Comparison - Test Perplexity (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars2:\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.2f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz_path = 'data/model_comparison.png'\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ… Visualization saved to: {viz_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Figure 2: Training Time Comparison\n",
    "    fig2, ax3 = plt.subplots(figsize=(10, 6))\n",
    "    train_times = df_plot['Training_Time']\n",
    "    bars3 = ax3.barh(models, train_times, color='lightgreen')\n",
    "    ax3.set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "    ax3.set_title('Model Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for bar in bars3:\n",
    "        width = bar.get_width()\n",
    "        ax3.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.1f}s', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    time_viz_path = 'data/training_time_comparison.png'\n",
    "    plt.savefig(time_viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… Training time visualization saved to: {time_viz_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâš ï¸  No data available for visualization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¯ PROJECT COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ“ All results saved to data/ directory:\")\n",
    "print(\"  â€¢ experiment_results.csv - Core experiment metrics\")\n",
    "print(\"  â€¢ text_generation_results.csv - Generated poetry samples\")\n",
    "print(\"  â€¢ model_comparison.png - Accuracy & Perplexity charts\")\n",
    "print(\"  â€¢ training_time_comparison.png - Training duration analysis\")\n",
    "print(\"\\nâœ… Semester project notebook successfully completed!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8806621,
     "sourceId": 13828071,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
